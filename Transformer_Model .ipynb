{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFets07dOCEZ"
      },
      "source": [
        "# Neural Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytJOm71IOXax"
      },
      "source": [
        "## Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZdWHVF0N5nC",
        "outputId": "2c5c0505-233f-4c45-b293-e649ea3ea26b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/Neural Machine Translation/')\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICOxfIbiN591"
      },
      "source": [
        "## Load tokenizer model For English and Spanish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xstx3Ov_bGw",
        "outputId": "b7c5aab8-5cb4-4f30-a891-88041e77b2b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-08 13:58:36.199872: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2022-08-08 13:58:45.812593: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.9.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 32.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 41.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.12.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 37.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=1d7b906134371b0361a68e1db6f5b26abe0658bad1f45d9a92f69fee16b45f77\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.21.1\n"
          ]
        }
      ],
      "source": [
        "! python -m spacy download en_core_web_sm\n",
        "! python -m spacy download es_core_news_sm\n",
        "! pip install rouge\n",
        "! pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTsYLmXrOLkq"
      },
      "source": [
        "## Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OBbY4NfOLMK"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import tensor\n",
        "from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "from rouge import Rouge\n",
        "from testing import Testing\n",
        "import math\n",
        "import io \n",
        "import time\n",
        "import torch\n",
        "\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tSppxc0OaoW"
      },
      "source": [
        "## Importing and Tokenizing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyiWgavHOlJH"
      },
      "source": [
        "### Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLOIKCRSOj7F"
      },
      "outputs": [],
      "source": [
        "names = ['english', 'spanish', 'version_details']\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Neural Machine Translation/Dataset/spa.txt\", delimiter='\\t', names=names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q_PmZkHOrmO"
      },
      "source": [
        "### Splitting dataset into Train, Val and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nkqOeY7OwJG"
      },
      "outputs": [],
      "source": [
        "en_train, en_test, es_train, es_test = train_test_split(dataset['english'], dataset['spanish'], test_size=0.2, random_state=1)\n",
        "en_train, en_val, es_train, es_val = train_test_split(en_train, es_train, test_size=0.25, random_state=1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RAnptdsOnQq"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T2UsuyUOzZM"
      },
      "source": [
        "#### Loading Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCeoLah4Om8R"
      },
      "outputs": [],
      "source": [
        "es_tokenizer = get_tokenizer('spacy', language='es_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDfqv_xjO3mB"
      },
      "source": [
        "#### Building Vocabulary and creating data tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOaRAqTlPKp2"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(data, tokenizer):\n",
        "    counter = Counter()\n",
        "    for datapoint in data:\n",
        "        # print(datapoint)\n",
        "        counter.update(tokenizer(datapoint))\n",
        "    return vocab(counter, specials=['<unk>','<pad>','<bos>','<eos>'])\n",
        "\n",
        "es_vocab = build_vocabulary(es_train, es_tokenizer)\n",
        "en_vocab = build_vocabulary(en_train, en_tokenizer)\n",
        "\n",
        "def process(en, es):\n",
        "    data = []\n",
        "    for raw_en, raw_es in zip(en,es):\n",
        "        es_tensor = torch.tensor([es_vocab[token] if token in es_vocab else es_vocab['<unk>'] for token in es_tokenizer(raw_es)], dtype=torch.long)\n",
        "        en_tensor = torch.tensor([en_vocab[token] if token in en_vocab else en_vocab['<unk>'] for token in en_tokenizer(raw_en)], dtype=torch.long)\n",
        "        data.append((es_tensor, en_tensor))\n",
        "    return data\n",
        "\n",
        "train_data = process(en_train, es_train)\n",
        "val_data = process(en_val, es_val)\n",
        "test_data = process(en_test, es_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN12CdwcPTYd"
      },
      "source": [
        "### Batch Generation and Padding sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_18eYCaPVTJ"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "PAD_IDX = es_vocab['<pad>']\n",
        "BOS_IDX = es_vocab['<bos>']\n",
        "EOS_IDX = es_vocab['<eos>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow9mKboJPZqm"
      },
      "outputs": [],
      "source": [
        "def batch_generator(data):\n",
        "    es_batch, en_batch = [], []\n",
        "    for es_item, en_item in data:\n",
        "        es_batch.append(torch.cat([torch.tensor([BOS_IDX]), es_item, torch.tensor([EOS_IDX])]))\n",
        "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])]))\n",
        "    es_batch = pad_sequence(es_batch, padding_value=PAD_IDX)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "    return es_batch, en_batch\n",
        "\n",
        "train_iter = DataLoader(train_data,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle = True, \n",
        "    collate_fn= batch_generator\n",
        "    )\n",
        "\n",
        "val_iter = DataLoader(val_data,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle = True, \n",
        "    collate_fn= batch_generator\n",
        "    )\n",
        "\n",
        "test_iter = DataLoader(test_data,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle = True, \n",
        "    collate_fn= batch_generator\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrNbl4izPfLm"
      },
      "source": [
        "## Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AaKCcaoPmnQ"
      },
      "source": [
        "### Global Parameters "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-8ir_uPPl8d"
      },
      "outputs": [],
      "source": [
        "src_vocab_len = len(es_vocab)\n",
        "tgt_vocab_len = len(en_vocab)\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "hdn_dim = 512\n",
        "BATCH_SIZE = 128\n",
        "num_encoder_layers= 3\n",
        "num_decoder_layers = 3\n",
        "epochs = 50\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDM-z5u6Pu2C"
      },
      "source": [
        "### Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaNWuR8ZPy-q"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
        "                 emb_size: int, src_vocab_len: int, tgt_vocab_len: int,\n",
        "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
        "                                                dim_feedforward=dim_feedforward)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_len)\n",
        "        self.src_tok_emb = TokenEmbedder(src_vocab_len, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedder(tgt_vocab_len, emb_size)\n",
        "        self.positional_encoding = PositionalEncoderg(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self, src: tensor, trg: tensor, src_mask: tensor,\n",
        "                tgt_mask: tensor, src_padding_mask: tensor,\n",
        "                tgt_padding_mask: tensor, memory_key_padding_mask: tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
        "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
        "                                        tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: tensor, src_mask: tensor):\n",
        "        return self.transformer_encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: tensor, memory: tensor, tgt_mask: tensor):\n",
        "        return self.transformer_decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oALo7uiP0o4"
      },
      "source": [
        "### Token Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptzOKIkP3DI"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baM8StFRP4dx"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE9o0gPJP7JI"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoderg(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
        "        super(PositionalEncoderg, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_emb = torch.zeros((maxlen, emb_size))\n",
        "        pos_emb[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_emb[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_emb = pos_emb.unsqueeze(-2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_emb', pos_emb)\n",
        "\n",
        "    def forward(self, token_embedding: tensor):\n",
        "        return self.dropout(token_embedding +\n",
        "                            self.pos_emb[:token_embedding.size(0),:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZZZtyddP8q8"
      },
      "source": [
        "### Mask Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QI_8LgTP_Gu"
      },
      "outputs": [],
      "source": [
        "def subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc5DVP9sQES3"
      },
      "source": [
        "### Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw3kz4RQQBW-"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(NUM_ENCODER_LAYERS, num_decoder_layers,\n",
        "                                 EMB_SIZE, src_vocab_len, tgt_vocab_len,\n",
        "                                 hdn_dim)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWZBz7GEQLbo"
      },
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "The Trained model is saved in models folder of parent folder NMT using Transformer Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkWf91bxQO3W"
      },
      "outputs": [],
      "source": [
        "def train(model, train_iter, optimizer):\n",
        "    model.train()\n",
        "    loss = 0\n",
        "    for idx, (src, tgt) in enumerate(train_iter):\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                       src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        loss += loss.item()\n",
        "        \n",
        "    torch.save(model, \"/content/drive/MyDrive/Neural Machine Translation/NMT using Transformer Model/Models/t-nmt-model.pt\")\n",
        "\n",
        "    return loss / len(train_iter)\n",
        "\n",
        "\n",
        "def evaluate(model, val_iter):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    for idx, (src, tgt) in (enumerate(val_iter)):\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                       src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss += loss.item()\n",
        "    return loss / len(val_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyHLFdFeQRCO",
        "outputId": "805b1aaf-1d03-40b4-83b5-e3f07c907b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train loss: 6.827, Val loss: 5.309, Epoch time = 81.275s\n",
            "Epoch: 2, Train loss: 4.950, Val loss: 4.590, Epoch time = 80.897s\n",
            "Epoch: 3, Train loss: 4.436, Val loss: 4.171, Epoch time = 80.883s\n",
            "Epoch: 4, Train loss: 4.088, Val loss: 3.874, Epoch time = 80.646s\n",
            "Epoch: 5, Train loss: 3.833, Val loss: 3.651, Epoch time = 80.931s\n",
            "Epoch: 6, Train loss: 3.632, Val loss: 3.462, Epoch time = 81.208s\n",
            "Epoch: 7, Train loss: 3.461, Val loss: 3.307, Epoch time = 82.268s\n",
            "Epoch: 8, Train loss: 3.311, Val loss: 3.160, Epoch time = 80.874s\n",
            "Epoch: 9, Train loss: 3.179, Val loss: 3.040, Epoch time = 81.418s\n",
            "Epoch: 10, Train loss: 3.058, Val loss: 2.933, Epoch time = 80.758s\n",
            "Epoch: 11, Train loss: 2.949, Val loss: 2.825, Epoch time = 81.125s\n",
            "Epoch: 12, Train loss: 2.847, Val loss: 2.737, Epoch time = 80.603s\n",
            "Epoch: 13, Train loss: 2.751, Val loss: 2.650, Epoch time = 80.706s\n",
            "Epoch: 14, Train loss: 2.662, Val loss: 2.568, Epoch time = 80.895s\n",
            "Epoch: 15, Train loss: 2.579, Val loss: 2.490, Epoch time = 80.922s\n",
            "Epoch: 16, Train loss: 2.498, Val loss: 2.416, Epoch time = 80.845s\n",
            "Epoch: 17, Train loss: 2.422, Val loss: 2.349, Epoch time = 80.804s\n",
            "Epoch: 18, Train loss: 2.351, Val loss: 2.291, Epoch time = 80.931s\n",
            "Epoch: 19, Train loss: 2.283, Val loss: 2.233, Epoch time = 81.145s\n",
            "Epoch: 20, Train loss: 2.218, Val loss: 2.175, Epoch time = 80.645s\n",
            "Epoch: 21, Train loss: 2.158, Val loss: 2.136, Epoch time = 80.950s\n",
            "Epoch: 22, Train loss: 2.100, Val loss: 2.079, Epoch time = 81.029s\n",
            "Epoch: 23, Train loss: 2.045, Val loss: 2.042, Epoch time = 80.988s\n",
            "Epoch: 24, Train loss: 1.992, Val loss: 1.992, Epoch time = 80.409s\n",
            "Epoch: 25, Train loss: 1.942, Val loss: 1.956, Epoch time = 80.733s\n",
            "Epoch: 26, Train loss: 1.895, Val loss: 1.918, Epoch time = 80.454s\n",
            "Epoch: 27, Train loss: 1.849, Val loss: 1.880, Epoch time = 82.098s\n",
            "Epoch: 28, Train loss: 1.805, Val loss: 1.847, Epoch time = 80.821s\n",
            "Epoch: 29, Train loss: 1.763, Val loss: 1.816, Epoch time = 80.886s\n",
            "Epoch: 30, Train loss: 1.725, Val loss: 1.788, Epoch time = 81.150s\n",
            "Epoch: 31, Train loss: 1.686, Val loss: 1.759, Epoch time = 81.354s\n",
            "Epoch: 32, Train loss: 1.650, Val loss: 1.732, Epoch time = 81.209s\n",
            "Epoch: 33, Train loss: 1.616, Val loss: 1.712, Epoch time = 80.672s\n",
            "Epoch: 34, Train loss: 1.582, Val loss: 1.685, Epoch time = 81.031s\n",
            "Epoch: 35, Train loss: 1.550, Val loss: 1.664, Epoch time = 80.926s\n",
            "Epoch: 36, Train loss: 1.519, Val loss: 1.638, Epoch time = 80.557s\n",
            "Epoch: 37, Train loss: 1.490, Val loss: 1.625, Epoch time = 81.096s\n",
            "Epoch: 38, Train loss: 1.462, Val loss: 1.606, Epoch time = 80.683s\n",
            "Epoch: 39, Train loss: 1.434, Val loss: 1.585, Epoch time = 81.096s\n",
            "Epoch: 40, Train loss: 1.408, Val loss: 1.573, Epoch time = 80.905s\n",
            "Epoch: 41, Train loss: 1.382, Val loss: 1.552, Epoch time = 80.890s\n",
            "Epoch: 42, Train loss: 1.358, Val loss: 1.542, Epoch time = 80.786s\n",
            "Epoch: 43, Train loss: 1.335, Val loss: 1.521, Epoch time = 80.838s\n",
            "Epoch: 44, Train loss: 1.310, Val loss: 1.507, Epoch time = 80.768s\n",
            "Epoch: 45, Train loss: 1.290, Val loss: 1.495, Epoch time = 81.065s\n",
            "Epoch: 46, Train loss: 1.267, Val loss: 1.482, Epoch time = 80.775s\n",
            "Epoch: 47, Train loss: 1.246, Val loss: 1.471, Epoch time = 80.964s\n",
            "Epoch: 48, Train loss: 1.225, Val loss: 1.463, Epoch time = 80.486s\n",
            "Epoch: 49, Train loss: 1.205, Val loss: 1.451, Epoch time = 80.663s\n",
            "Epoch: 50, Train loss: 1.188, Val loss: 1.439, Epoch time = 81.303s\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, epochs+1):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(transformer, train_iter, optimizer)\n",
        "    end_time = time.time()\n",
        "    val_loss = evaluate(transformer, val_iter)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
        "           f\"Epoch time = {(end_time - start_time):.3f}s\"))       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HoLbRkRRomV"
      },
      "source": [
        "## Translating Sentences from Spanish to English:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab3OHkxZRImF"
      },
      "source": [
        "### Greedy Decode and Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C01QjneRMTK"
      },
      "outputs": [],
      "source": [
        "def decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask.to(device)\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(device)\n",
        "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
        "        tgt_mask = (subsequent_mask(ys.size(0))\n",
        "                                    .type(torch.bool)).to(device)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "          break\n",
        "    return ys\n",
        "\n",
        "\n",
        "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
        "\n",
        "    model.eval()\n",
        "    tokens = [BOS_IDX] + [src_vocab.__getitem__(tok) if src_vocab.__contains__(tok) else src_vocab.__getitem__(\"<unk>\") for tok in src_tokenizer(src)] + [EOS_IDX]\n",
        "    num_tokens = len(tokens)\n",
        "    src = (torch.LongTensor(tokens).reshape(num_tokens, 1))\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = decode(model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    # print()\n",
        "    return \" \".join([tgt_vocab.lookup_token(tok.item()) for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDLmJc1YJm_M"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from string import digits\n",
        "def preprocess_for_nmt(sent):\n",
        "\n",
        "  # convert text to lower case\n",
        "  sent = sent.lower()\n",
        "\n",
        "  # removing unnecessarily long spaces \n",
        "  sent = re.sub(\" +\", \" \", sent)\n",
        "\n",
        "  # removing quotes\n",
        "  sent = re.sub(\"'\", \"\", sent)\n",
        "\n",
        "  # replacing digits with none: in this --> \"str.maketrans('','', digits)\" ------  returns a map for ascii codes of digits 0 through 9, that is to be replaced by None\n",
        "  sent = sent.translate(str.maketrans('','', digits))\n",
        "\n",
        "  # Adding spaces before and after punctuations (issue : The list of puctuation is from the reference I am using, when I add the list, it throws multiple errors)\n",
        "  sent = re.sub(r\"([?.!,¿])\", r\" \\1 \", sent)\n",
        "\n",
        "  # Strip the white spaces\n",
        "  sent = sent.strip()\n",
        "\n",
        "  return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ONyKySItm23"
      },
      "outputs": [],
      "source": [
        "def load_model():\n",
        "  model = torch.load('/content/drive/MyDrive/Neural Machine Translation/NMT using Transformer Model/Models/Final_model.pt', map_location=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpSUdlkns06L"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clnI9zWSs0kJ"
      },
      "outputs": [],
      "source": [
        "model = load_model()\n",
        "expected = en_test\n",
        "pred = [translate( model , sentence, es_vocab, en_vocab, es_tokenizer ) for sentence in es_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP2WBK7Tw6ma",
        "outputId": "d3ceea73-d93f-4bd1-e952-436306cf226a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.6250791\n",
            "Recall Score: 0.6424230\n",
            "F1 Score: 0.6451164\n",
            "Bleu Score: 0.5540225\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from testing import Testing\n",
        "! pip install sentence_transformers\n",
        "! pip install rouge\n",
        "test = Testing(pred, expected)\n",
        "test.score()\n",
        "print(\"Precision: \",test.precision)\n",
        "print(\"Recall Score: \",test.recall)\n",
        "print(\"F1 Score: \",test.f1)\n",
        "print(\"Bleu Score: \",test.bleu)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "ytJOm71IOXax",
        "ICOxfIbiN591",
        "lTsYLmXrOLkq",
        "7tSppxc0OaoW",
        "jyiWgavHOlJH",
        "2Q_PmZkHOrmO",
        "jrNbl4izPfLm",
        "-ZZZtyddP8q8",
        "dWZBz7GEQLbo",
        "1HoLbRkRRomV"
      ],
      "name": "Transformer Model",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
