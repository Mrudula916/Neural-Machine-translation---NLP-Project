# -*- coding: utf-8 -*-
"""Testing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zPRq2sJIecTcEMIIeYXle2Zd2tm_fPET
"""

from rouge import Rouge
from nltk.translate.bleu_score import sentence_bleu
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

import warnings
warnings.filterwarnings('ignore')

class Testing():
  def __init__(self, predicted, expected):
    self.predicted = [predict.lower() for predict in predicted]
    self.expected = [expect.lower() for expect in expected]
    # self.accuracy = None
    self.f1 = None
    self.precision = None
    self.recall = None
    self.bleu = None
  
  def _blue_format_(self, sentences):
    res = [sentence.lower().split() for sentence in sentences]
    print(res)
    return res

  def score(self):
    metrics = Rouge()
    metrics = metrics.get_scores(list(self.predicted), list(self.expected), avg=True)
    self.f1 = sum([metrics[key]['f'] for key in list(metrics.keys())])/3
    self.recall = sum([metrics[key]['r'] for key in list(metrics.keys())])/3
    self.precision = sum([metrics[key]['p'] for key in list(metrics.keys())])/3
    self.bleu = [sentence_bleu(exp, pred) for exp,pred in zip([self._blue_format_(self.expected)],self._blue_format_(self.predicted))]
    self.bleu = sum(self.bleu)/len(self.bleu)