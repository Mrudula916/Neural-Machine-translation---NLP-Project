{"cells":[{"cell_type":"markdown","metadata":{"id":"VFets07dOCEZ"},"source":["# Neural Machine Translation"]},{"cell_type":"markdown","metadata":{"id":"ytJOm71IOXax"},"source":["## Drive Mount"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":773,"status":"ok","timestamp":1659971091916,"user":{"displayName":"Aanand Dhandapani","userId":"04899665584356915787"},"user_tz":240},"id":"jZdWHVF0N5nC","outputId":"2c5c0505-233f-4c45-b293-e649ea3ea26b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","import sys\n","sys.path.insert(0,'/content/drive/MyDrive/Neural Machine Translation/')\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ICOxfIbiN591"},"source":["## Load tokenizer model For English and Spanish"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39021,"status":"ok","timestamp":1659967146384,"user":{"displayName":"Aanand Dhandapani","userId":"04899665584356915787"},"user_tz":240},"id":"0xstx3Ov_bGw","outputId":"b7c5aab8-5cb4-4f30-a891-88041e77b2b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["2022-08-08 13:58:36.199872: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","2022-08-08 13:58:45.812593: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting es-core-news-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n","\u001b[K     |████████████████████████████████| 12.9 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.9.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.0)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: rouge in /usr/local/lib/python3.7/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentence_transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[K     |████████████████████████████████| 85 kB 2.4 MB/s \n","\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 32.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.0+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.0+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 41.7 MB/s \n","\u001b[?25hCollecting huggingface-hub>=0.4.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 8.2 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 58.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.7.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.12.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 37.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=1d7b906134371b0361a68e1db6f5b26abe0658bad1f45d9a92f69fee16b45f77\n","  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n","Successfully built sentence-transformers\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.21.1\n"]}],"source":["! python -m spacy download en_core_web_sm\n","! python -m spacy download es_core_news_sm\n","! pip install rouge\n","! pip install sentence_transformers"]},{"cell_type":"markdown","metadata":{"id":"lTsYLmXrOLkq"},"source":["## Necessary Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-OBbY4NfOLMK"},"outputs":[],"source":["from torch import nn\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","from torch import tensor\n","from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n","\n","from torchtext.data.utils import get_tokenizer\n","from collections import Counter\n","from torchtext.vocab import vocab\n","\n","from sklearn.model_selection import train_test_split \n","from rouge import Rouge\n","from testing import Testing\n","import math\n","import io \n","import time\n","import torch\n","\n","import pandas as pd\n"]},{"cell_type":"markdown","metadata":{"id":"7tSppxc0OaoW"},"source":["## Importing and Tokenizing Dataset"]},{"cell_type":"markdown","metadata":{"id":"jyiWgavHOlJH"},"source":["### Importing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLOIKCRSOj7F"},"outputs":[],"source":["names = ['english', 'spanish', 'version_details']\n","dataset = pd.read_csv(\"/content/drive/MyDrive/Neural Machine Translation/Dataset/spa.txt\", delimiter='\\t', names=names)"]},{"cell_type":"markdown","metadata":{"id":"2Q_PmZkHOrmO"},"source":["### Splitting dataset into Train, Val and Test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_nkqOeY7OwJG"},"outputs":[],"source":["en_train, en_test, es_train, es_test = train_test_split(dataset['english'], dataset['spanish'], test_size=0.2, random_state=1)\n","en_train, en_val, es_train, es_val = train_test_split(en_train, es_train, test_size=0.25, random_state=1) "]},{"cell_type":"markdown","metadata":{"id":"6RAnptdsOnQq"},"source":["### Tokenizing"]},{"cell_type":"markdown","metadata":{"id":"0T2UsuyUOzZM"},"source":["#### Loading Tokenizers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCeoLah4Om8R"},"outputs":[],"source":["es_tokenizer = get_tokenizer('spacy', language='es_core_news_sm')\n","en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"]},{"cell_type":"markdown","metadata":{"id":"rDfqv_xjO3mB"},"source":["#### Building Vocabulary and creating data tensors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mOaRAqTlPKp2"},"outputs":[],"source":["def build_vocabulary(data, tokenizer):\n","    counter = Counter()\n","    for datapoint in data:\n","        # print(datapoint)\n","        counter.update(tokenizer(datapoint))\n","    return vocab(counter, specials=['<unk>','<pad>','<bos>','<eos>'])\n","\n","es_vocab = build_vocabulary(es_train, es_tokenizer)\n","en_vocab = build_vocabulary(en_train, en_tokenizer)\n","\n","def process(en, es):\n","    data = []\n","    for raw_en, raw_es in zip(en,es):\n","        es_tensor = torch.tensor([es_vocab[token] if token in es_vocab else es_vocab['<unk>'] for token in es_tokenizer(raw_es)], dtype=torch.long)\n","        en_tensor = torch.tensor([en_vocab[token] if token in en_vocab else en_vocab['<unk>'] for token in en_tokenizer(raw_en)], dtype=torch.long)\n","        data.append((es_tensor, en_tensor))\n","    return data\n","\n","train_data = process(en_train, es_train)\n","val_data = process(en_val, es_val)\n","test_data = process(en_test, es_test)"]},{"cell_type":"markdown","metadata":{"id":"EN12CdwcPTYd"},"source":["### Batch Generation and Padding sentences\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_18eYCaPVTJ"},"outputs":[],"source":["BATCH_SIZE = 128\n","PAD_IDX = es_vocab['<pad>']\n","BOS_IDX = es_vocab['<bos>']\n","EOS_IDX = es_vocab['<eos>']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ow9mKboJPZqm"},"outputs":[],"source":["def batch_generator(data):\n","    es_batch, en_batch = [], []\n","    for es_item, en_item in data:\n","        es_batch.append(torch.cat([torch.tensor([BOS_IDX]), es_item, torch.tensor([EOS_IDX])]))\n","        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])]))\n","    es_batch = pad_sequence(es_batch, padding_value=PAD_IDX)\n","    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n","    return es_batch, en_batch\n","\n","train_iter = DataLoader(train_data,\n","    batch_size = BATCH_SIZE,\n","    shuffle = True, \n","    collate_fn= batch_generator\n","    )\n","\n","val_iter = DataLoader(val_data,\n","    batch_size = BATCH_SIZE,\n","    shuffle = True, \n","    collate_fn= batch_generator\n","    )\n","\n","test_iter = DataLoader(test_data,\n","    batch_size = BATCH_SIZE,\n","    shuffle = True, \n","    collate_fn= batch_generator\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RNNpERyxcDZo"},"outputs":[],"source":["# es_vocab.get_stoi()\n","# es_vocab.lookup_token([1])"]},{"cell_type":"markdown","metadata":{"id":"jrNbl4izPfLm"},"source":["## Neural Network Model"]},{"cell_type":"markdown","metadata":{"id":"7AaKCcaoPmnQ"},"source":["### Global Parameters "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-8ir_uPPl8d"},"outputs":[],"source":["SRC_VOCAB_SIZE = len(es_vocab)\n","TGT_VOCAB_SIZE = len(en_vocab)\n","EMB_SIZE = 512\n","NHEAD = 8\n","FFN_HID_DIM = 512\n","BATCH_SIZE = 128\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","NUM_EPOCHS = 50\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"yDM-z5u6Pu2C"},"source":["### Transformer Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaNWuR8ZPy-q"},"outputs":[],"source":["class Seq2SeqTransformer(nn.Module):\n","    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n","                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n","                 dim_feedforward:int = 512, dropout:float = 0.1):\n","        super(Seq2SeqTransformer, self).__init__()\n","        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n","                                                dim_feedforward=dim_feedforward)\n","        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n","        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n","                                                dim_feedforward=dim_feedforward)\n","        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n","        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n","        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n","        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n","        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n","\n","    def forward(self, src: tensor, trg: tensor, src_mask: tensor,\n","                tgt_mask: tensor, src_padding_mask: tensor,\n","                tgt_padding_mask: tensor, memory_key_padding_mask: tensor):\n","        src_emb = self.positional_encoding(self.src_tok_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n","        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n","        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n","                                        tgt_padding_mask, memory_key_padding_mask)\n","        return self.generator(outs)\n","\n","    def encode(self, src: tensor, src_mask: tensor):\n","        return self.transformer_encoder(self.positional_encoding(\n","                            self.src_tok_emb(src)), src_mask)\n","\n","    def decode(self, tgt: tensor, memory: tensor, tgt_mask: tensor):\n","        return self.transformer_decoder(self.positional_encoding(\n","                          self.tgt_tok_emb(tgt)), memory,\n","                          tgt_mask)"]},{"cell_type":"markdown","metadata":{"id":"1oALo7uiP0o4"},"source":["### Token Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ptzOKIkP3DI"},"outputs":[],"source":["class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size: int, emb_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens: tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"]},{"cell_type":"markdown","metadata":{"id":"baM8StFRP4dx"},"source":["### Positional Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IE9o0gPJP7JI"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding: tensor):\n","        return self.dropout(token_embedding +\n","                            self.pos_embedding[:token_embedding.size(0),:])"]},{"cell_type":"markdown","metadata":{"id":"-ZZZtyddP8q8"},"source":["### Mask Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QI_8LgTP_Gu"},"outputs":[],"source":["def generate_square_subsequent_mask(sz):\n","    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","def create_mask(src, tgt):\n","    src_seq_len = src.shape[0]\n","    tgt_seq_len = tgt.shape[0]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n","    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n","\n","    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n","    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"]},{"cell_type":"markdown","metadata":{"id":"qc5DVP9sQES3"},"source":["### Loss Function and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jw3kz4RQQBW-"},"outputs":[],"source":["transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n","                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n","                                 FFN_HID_DIM)\n","\n","for p in transformer.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","transformer = transformer.to(device)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(\n","    transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9\n",")"]},{"cell_type":"markdown","metadata":{"id":"dWZBz7GEQLbo"},"source":["## Training and Evaluation\n","\n","The Trained model is saved in models folder of parent folder NMT using Transformer Model\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkWf91bxQO3W"},"outputs":[],"source":["def train_epoch(model, train_iter, optimizer):\n","    model.train()\n","    losses = 0\n","    for idx, (src, tgt) in enumerate(train_iter):\n","        src = src.to(device)\n","        tgt = tgt.to(device)\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,\n","                       src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        optimizer.zero_grad()\n","\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","        \n","    torch.save(model, \"/content/drive/MyDrive/Neural Machine Translation/NMT using Transformer Model/Models/t-nmt-model.pt\")\n","\n","    return losses / len(train_iter)\n","\n","\n","def evaluate(model, val_iter):\n","    model.eval()\n","    losses = 0\n","    for idx, (src, tgt) in (enumerate(val_iter)):\n","        src = src.to(device)\n","        tgt = tgt.to(device)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,\n","                       src_padding_mask, tgt_padding_mask, src_padding_mask)\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        losses += loss.item()\n","    return losses / len(val_iter)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4511521,"status":"ok","timestamp":1659906240199,"user":{"displayName":"Aanand Dhandapani","userId":"04899665584356915787"},"user_tz":240},"id":"WyHLFdFeQRCO","outputId":"805b1aaf-1d03-40b4-83b5-e3f07c907b3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1, Train loss: 6.827, Val loss: 5.309, Epoch time = 81.275s\n","Epoch: 2, Train loss: 4.950, Val loss: 4.590, Epoch time = 80.897s\n","Epoch: 3, Train loss: 4.436, Val loss: 4.171, Epoch time = 80.883s\n","Epoch: 4, Train loss: 4.088, Val loss: 3.874, Epoch time = 80.646s\n","Epoch: 5, Train loss: 3.833, Val loss: 3.651, Epoch time = 80.931s\n","Epoch: 6, Train loss: 3.632, Val loss: 3.462, Epoch time = 81.208s\n","Epoch: 7, Train loss: 3.461, Val loss: 3.307, Epoch time = 82.268s\n","Epoch: 8, Train loss: 3.311, Val loss: 3.160, Epoch time = 80.874s\n","Epoch: 9, Train loss: 3.179, Val loss: 3.040, Epoch time = 81.418s\n","Epoch: 10, Train loss: 3.058, Val loss: 2.933, Epoch time = 80.758s\n","Epoch: 11, Train loss: 2.949, Val loss: 2.825, Epoch time = 81.125s\n","Epoch: 12, Train loss: 2.847, Val loss: 2.737, Epoch time = 80.603s\n","Epoch: 13, Train loss: 2.751, Val loss: 2.650, Epoch time = 80.706s\n","Epoch: 14, Train loss: 2.662, Val loss: 2.568, Epoch time = 80.895s\n","Epoch: 15, Train loss: 2.579, Val loss: 2.490, Epoch time = 80.922s\n","Epoch: 16, Train loss: 2.498, Val loss: 2.416, Epoch time = 80.845s\n","Epoch: 17, Train loss: 2.422, Val loss: 2.349, Epoch time = 80.804s\n","Epoch: 18, Train loss: 2.351, Val loss: 2.291, Epoch time = 80.931s\n","Epoch: 19, Train loss: 2.283, Val loss: 2.233, Epoch time = 81.145s\n","Epoch: 20, Train loss: 2.218, Val loss: 2.175, Epoch time = 80.645s\n","Epoch: 21, Train loss: 2.158, Val loss: 2.136, Epoch time = 80.950s\n","Epoch: 22, Train loss: 2.100, Val loss: 2.079, Epoch time = 81.029s\n","Epoch: 23, Train loss: 2.045, Val loss: 2.042, Epoch time = 80.988s\n","Epoch: 24, Train loss: 1.992, Val loss: 1.992, Epoch time = 80.409s\n","Epoch: 25, Train loss: 1.942, Val loss: 1.956, Epoch time = 80.733s\n","Epoch: 26, Train loss: 1.895, Val loss: 1.918, Epoch time = 80.454s\n","Epoch: 27, Train loss: 1.849, Val loss: 1.880, Epoch time = 82.098s\n","Epoch: 28, Train loss: 1.805, Val loss: 1.847, Epoch time = 80.821s\n","Epoch: 29, Train loss: 1.763, Val loss: 1.816, Epoch time = 80.886s\n","Epoch: 30, Train loss: 1.725, Val loss: 1.788, Epoch time = 81.150s\n","Epoch: 31, Train loss: 1.686, Val loss: 1.759, Epoch time = 81.354s\n","Epoch: 32, Train loss: 1.650, Val loss: 1.732, Epoch time = 81.209s\n","Epoch: 33, Train loss: 1.616, Val loss: 1.712, Epoch time = 80.672s\n","Epoch: 34, Train loss: 1.582, Val loss: 1.685, Epoch time = 81.031s\n","Epoch: 35, Train loss: 1.550, Val loss: 1.664, Epoch time = 80.926s\n","Epoch: 36, Train loss: 1.519, Val loss: 1.638, Epoch time = 80.557s\n","Epoch: 37, Train loss: 1.490, Val loss: 1.625, Epoch time = 81.096s\n","Epoch: 38, Train loss: 1.462, Val loss: 1.606, Epoch time = 80.683s\n","Epoch: 39, Train loss: 1.434, Val loss: 1.585, Epoch time = 81.096s\n","Epoch: 40, Train loss: 1.408, Val loss: 1.573, Epoch time = 80.905s\n","Epoch: 41, Train loss: 1.382, Val loss: 1.552, Epoch time = 80.890s\n","Epoch: 42, Train loss: 1.358, Val loss: 1.542, Epoch time = 80.786s\n","Epoch: 43, Train loss: 1.335, Val loss: 1.521, Epoch time = 80.838s\n","Epoch: 44, Train loss: 1.310, Val loss: 1.507, Epoch time = 80.768s\n","Epoch: 45, Train loss: 1.290, Val loss: 1.495, Epoch time = 81.065s\n","Epoch: 46, Train loss: 1.267, Val loss: 1.482, Epoch time = 80.775s\n","Epoch: 47, Train loss: 1.246, Val loss: 1.471, Epoch time = 80.964s\n","Epoch: 48, Train loss: 1.225, Val loss: 1.463, Epoch time = 80.486s\n","Epoch: 49, Train loss: 1.205, Val loss: 1.451, Epoch time = 80.663s\n","Epoch: 50, Train loss: 1.188, Val loss: 1.439, Epoch time = 81.303s\n"]}],"source":["for epoch in range(1, NUM_EPOCHS+1):\n","    start_time = time.time()\n","    train_loss = train_epoch(transformer, train_iter, optimizer)\n","    end_time = time.time()\n","    val_loss = evaluate(transformer, val_iter)\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n","           f\"Epoch time = {(end_time - start_time):.3f}s\"))       "]},{"cell_type":"markdown","metadata":{"id":"1HoLbRkRRomV"},"source":["## Translating Sentences from Spanish to English:\n"]},{"cell_type":"markdown","metadata":{"id":"ab3OHkxZRImF"},"source":["### Greedy Decode and Translation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3C01QjneRMTK"},"outputs":[],"source":["def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    src = src.to(device)\n","    src_mask = src_mask.to(device)\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n","    for i in range(max_len-1):\n","        memory = memory.to(device)\n","        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                                    .type(torch.bool)).to(device)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim = 1)\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","          break\n","    return ys\n","\n","\n","def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n","\n","    model.eval()\n","    tokens = [BOS_IDX] + [src_vocab.__getitem__(tok) if src_vocab.__contains__(tok) else src_vocab.__getitem__(\"<unk>\") for tok in src_tokenizer(src)] + [EOS_IDX]\n","    num_tokens = len(tokens)\n","    src = (torch.LongTensor(tokens).reshape(num_tokens, 1))\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n","    # print()\n","    return \" \".join([tgt_vocab.lookup_token(tok.item()) for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDLmJc1YJm_M"},"outputs":[],"source":["import re\n","from string import digits\n","def preprocess_for_nmt(sent):\n","\n","  # convert text to lower case\n","  sent = sent.lower()\n","\n","  # removing unnecessarily long spaces \n","  sent = re.sub(\" +\", \" \", sent)\n","\n","  # removing quotes\n","  sent = re.sub(\"'\", \"\", sent)\n","\n","  # replacing digits with none: in this --> \"str.maketrans('','', digits)\" ------  returns a map for ascii codes of digits 0 through 9, that is to be replaced by None\n","  sent = sent.translate(str.maketrans('','', digits))\n","\n","  # Adding spaces before and after punctuations (issue : The list of puctuation is from the reference I am using, when I add the list, it throws multiple errors)\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1 \", sent)\n","\n","  # Strip the white spaces\n","  sent = sent.strip()\n","\n","  return sent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ONyKySItm23"},"outputs":[],"source":["def load_model():\n","  model = torch.load('/content/drive/MyDrive/Neural Machine Translation/NMT using Transformer Model/Models/Final_model.pt', map_location=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'))\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWPF40prR1sl"},"outputs":[],"source":["# Requested_Sentence = input(\"Enter Sentence to Translate: \")\n","# Requested_sentence = \"¿Qué te gusta hacer?\"\n","\n","# Translated_Sentence = translate(load_model(), preprocess_for_nmt(Requested_sentence), es_vocab, en_vocab, es_tokenizer)\n","\n","# print(\"Translation: \", Translated_Sentence)"]},{"cell_type":"markdown","metadata":{"id":"ZpSUdlkns06L"},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clnI9zWSs0kJ"},"outputs":[],"source":["model = load_model()\n","expected = en_test\n","pred = [translate( model , sentence, es_vocab, en_vocab, es_tokenizer ) for sentence in es_test]"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NP2WBK7Tw6ma","executionInfo":{"status":"ok","timestamp":1660421780957,"user_tz":240,"elapsed":3,"user":{"displayName":"Aanand Dhandapani","userId":"04899665584356915787"}},"outputId":"d3ceea73-d93f-4bd1-e952-436306cf226a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.6250791\n","Recall Score: 0.6424230\n","F1 Score: 0.6451164\n","Bleu Score: 0.5540225\n"]}],"source":["\n","from testing import Testing\n","! pip install sentence_transformers\n","! pip install rouge\n","test = Testing(pred, expected)\n","test.score()\n","print(\"Precision: \",test.precision)\n","print(\"Recall Score: \",test.recall)\n","print(\"F1 Score: \",test.f1)\n","print(\"Bleu Score: \",test.bleu)"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":["ytJOm71IOXax","ICOxfIbiN591","lTsYLmXrOLkq","7tSppxc0OaoW","jyiWgavHOlJH","2Q_PmZkHOrmO","jrNbl4izPfLm","-ZZZtyddP8q8","dWZBz7GEQLbo","1HoLbRkRRomV"],"name":"Transformer Model","provenance":[],"authorship_tag":"ABX9TyOORfcWTxE7zPgAFo3RiY6Q"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}