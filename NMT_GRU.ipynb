{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NMT_GRU.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"_gFnnUZDLQBE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660424669483,"user_tz":240,"elapsed":16950,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}},"outputId":"f0e4a32b-6ec2-4277-bf48-74ef5a81b3dd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["\"\"\" ----------------- Importing Libraries --------------------------- \"\"\"\n","\n","\n","\n","from keras.preprocessing.text import Tokenizer\n","from collections import Counter\n","\n","import pandas as pd\n","import numpy as np\n","from string import digits\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","import nltk\n","import os\n","import re\n","import string\n","import io\n","from sklearn.model_selection import train_test_split\n","import pickle as pk\n","\n","\n","# to store trained objects \n","def serialize(path,data):\n","  pick = open(path, 'ab')\n","  pk.dump(data, pick)                     \n","  pick.close()\n","\n","# Retrieve the stored trained objects\n","def deserialize(path):\n","  pick = open(path, 'rb')     \n","  df = pk.load(pick)\n","  pick.close()\n","  return df"],"metadata":{"id":"OKmLnQ75LUAd","executionInfo":{"status":"ok","timestamp":1660424675995,"user_tz":240,"elapsed":6516,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\"\"\" ----------------- Loading the Data --------------------------- \"\"\"\n"," \n","path = '/content/gdrive/MyDrive/Neural Machine Translation/Dataset/spa.txt'\n","names = ['english', 'spanish', 'version_details']\n","dataset = pd.read_csv(path , delimiter='\\t', names=names)"],"metadata":{"id":"twVXRiwO-dlB","executionInfo":{"status":"ok","timestamp":1660424702094,"user_tz":240,"elapsed":1504,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\"\"\" ----------------- Preprocessing (specifically for nmt) --------------------------- \"\"\"\n","\n","def preprocess_for_nmt(sent):\n","\n","  # convert text to lower case\n","  sent = sent.lower()\n","\n","  # removing unnecessarily long spaces \n","  sent = re.sub(\" +\", \" \", sent)\n","\n","  # removing quotes\n","  sent = re.sub(\"'\", \"\", sent)\n","\n","  # replacing digits with none: in this --> \"str.maketrans('','', digits)\" ------  returns a map for ascii codes of digits 0 through 9, that is to be replaced by None\n","  sent = sent.translate(str.maketrans('','', digits))\n","  \n","  # Adding spaces before and after punctuations (issue : The list of puctuation is from the reference I am using, when I add the list, it throws multiple errors)\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1 \", sent)\n","\n","  # Strip the white spaces\n","  sent = sent.strip()\n","\n","  # appending start and end tokens since its required by the model to identify the start and end of the sequence\n","  sent = \"start_ \" + sent + \" _end\"\n","\n","  return sent"],"metadata":{"id":"qR60kZPULZEq","executionInfo":{"status":"ok","timestamp":1660424705904,"user_tz":240,"elapsed":543,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["dataset = dataset.loc[:96000]"],"metadata":{"id":"L0iziWh4PkNL","executionInfo":{"status":"ok","timestamp":1660424717925,"user_tz":240,"elapsed":161,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\"\"\" ----------------- Creating source target language pairs [source, target] --------------------------- \"\"\"\n","\n","def create_dataset(source, target):\n","  src = []\n","  trg = []\n","  for s,t in zip(source,target):\n","    src.append(preprocess_for_nmt(s))\n","    trg.append(preprocess_for_nmt(t))\n","\n","  return tuple(src), tuple(trg)\n","source, target = create_dataset(dataset.english, dataset.spanish)\n","\n","print(source[-1])\n","print(target[-1])\n","type(target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dE70RnAj_4vA","executionInfo":{"status":"ok","timestamp":1660424722227,"user_tz":240,"elapsed":2942,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}},"outputId":"7ef29626-f5b2-4fd4-9cda-83bee276fd7e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["start_ tom was killed by a suicide bomber . _end\n","start_ tom fue asesinado por un kamikaze . _end\n"]},{"output_type":"execute_result","data":{"text/plain":["tuple"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["\"\"\" ----------------- Source Tokenizer --------------------------- \"\"\"\n","\n","# create a tokenizer for source sentence\n","src_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","\n","# Fit the source sentences to the source tokenizer (creates an index to word map, which is then used to substitute in the texts_to_sequences method)\n","src_tokenizer.fit_on_texts(source)\n","\n","# Transforms each text in texts to a sequence of integers, and pad zeros to end of sentences to make the data instances uniform\n","src_tnsr = tf.keras.preprocessing.sequence.pad_sequences(src_tokenizer.texts_to_sequences(source),padding='post')\n","\n","src_vocab_length = len(src_tokenizer.word_index)+1"],"metadata":{"id":"InKxD9wujLYC","executionInfo":{"status":"ok","timestamp":1660424724611,"user_tz":240,"elapsed":1882,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["\"\"\" ----------------- Target Tokenizer --------------------------- \"\"\"\n","\n","# create a tokenizer for target sentence\n","trg_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","\n","# Fit the target sentences to the target tokenizer (creates an index to word map, which is then used to substitute in the texts_to_sequences method)\n","trg_tokenizer.fit_on_texts(target)\n","\n","# Transforms each text in texts to a sequence of integers, and pad zeros to end of sentences to make the data instances uniform\n","trg_tnsr = tf.keras.preprocessing.sequence.pad_sequences(trg_tokenizer.texts_to_sequences(target),padding='post')\n","\n","trg_vocab_length = len(trg_tokenizer.word_index)+1"],"metadata":{"id":"blpzZwtX_Pkf","executionInfo":{"status":"ok","timestamp":1660424726807,"user_tz":240,"elapsed":2044,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# split the dataset into training and testing sets\n","src_tnsr_tr, src_tnsr_ts, trg_tnsr_tr, trg_tnsr_ts = train_test_split(src_tnsr, trg_tnsr,test_size=0.2)"],"metadata":{"id":"dUYmvBQZAE84","executionInfo":{"status":"ok","timestamp":1660424728285,"user_tz":240,"elapsed":169,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# index to word map\n","def convert(tok, tnsr):\n","  for t in tnsr:\n","    if t!=0:\n","      print (\"%d ----> %s\" % (t, tok.index_word[t]))\n","\n","print (\"Input Language; index to word mapping\")\n","convert(src_tokenizer, src_tnsr_tr[0])\n","print ()\n","print (\"Target Language; index to word mapping\")\n","convert( trg_tokenizer, trg_tnsr_tr[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8FO2MI78cf5v","executionInfo":{"status":"ok","timestamp":1660424733278,"user_tz":240,"elapsed":162,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}},"outputId":"3e220360-394b-4d56-9e29-e06b9e5c54b9"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Language; index to word mapping\n","1 ----> start_\n","6 ----> tom\n","1040 ----> jumped\n","212 ----> over\n","9 ----> the\n","2649 ----> shallow\n","1789 ----> ditch\n","3 ----> .\n","2 ----> _end\n","\n","Target Language; index to word mapping\n","1 ----> start_\n","4 ----> tom\n","1326 ----> saltó\n","161 ----> sobre\n","12 ----> la\n","107 ----> poco\n","3151 ----> profunda\n","2611 ----> zanja\n","3 ----> .\n","2 ----> _end\n"]}]},{"cell_type":"code","source":["# Batch Size\n","batch = 64\n","\n","# Create dataset and shuffle them\n","dataset = tf.data.Dataset.from_tensor_slices((src_tnsr_tr, trg_tnsr_tr)).shuffle(batch)\n","\n","# Create the batches of 64 post shuffling\n","dataset = dataset.batch(batch, drop_remainder=True)\n","\n","# useful parameters\n","buffer = len(src_tnsr_tr)\n","steps_per_epoch= len(src_tnsr_tr)//batch\n","\n","embedding_dim=256\n","units=1024"],"metadata":{"id":"NZfk1NnNTVeA","executionInfo":{"status":"ok","timestamp":1660424743565,"user_tz":240,"elapsed":2985,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# iterator object is a pointer to an element in the dataset, initially set to the first element in subscriptable item\n","iterator = iter(dataset)\n","src_bat, trg_bat = next(iterator)\n","\n","print(src_bat.shape)\n","print(trg_bat.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IwEYXYGeXtSB","executionInfo":{"status":"ok","timestamp":1660424752161,"user_tz":240,"elapsed":176,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}},"outputId":"11eac8df-370b-4ce9-92bd-6f523f254022"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 14)\n","(64, 20)\n"]}]},{"cell_type":"code","source":["\n","print(\"Source language vocabulary length:-\",src_vocab_length)\n","print(\"Target language vocabulary length:-\",trg_vocab_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCuAg1nbYt_d","executionInfo":{"status":"ok","timestamp":1660424760638,"user_tz":240,"elapsed":158,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}},"outputId":"f7aa6a98-1b04-4e28-ce08-3710eda7caf8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Source language vocabulary length:- 10237\n","Target language vocabulary length:- 20506\n"]}]},{"cell_type":"code","source":["# an example, taking just one batch to experiment the algorithm\n","single_src, single_trg = next(iter(dataset))\n","single_src.shape, single_trg.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_QCutW9gq7c","executionInfo":{"status":"ok","timestamp":1660424763528,"user_tz":240,"elapsed":5,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}},"outputId":"2133ea48-f152-4754-8f25-ff8aaf8645d9"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 14]), TensorShape([64, 20]))"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Encoder architecture\n","\n","class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n","    super(Encoder, self).__init__()\n","    self.batch_size = batch_size\n","    self.enc_units = enc_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","\n","  def call(self, x, hidden):\n","    x = self.embedding(x)\n","    output, state = self.gru(x, initial_state = hidden)\n","    return output, state\n","\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_size, self.enc_units))\n","\n","\n","encoder = Encoder(src_vocab_length, embedding_dim, units, batch)\n","\n","# sample input\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(single_src, sample_hidden)\n","print (\"Output shape : \",sample_output.shape)\n","print (\"Hidden state : \",sample_hidden.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BJrqkdXNgd7b","executionInfo":{"status":"ok","timestamp":1660424814376,"user_tz":240,"elapsed":2716,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}},"outputId":"134dd9fa-2d53-4164-8542-ab8930d58dc0"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape :  (64, 14, 1024)\n","Hidden state :  (64, 1024)\n"]}]},{"cell_type":"code","source":["# Attention Mechanism\n","\n","class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # hidden shape == (batch_size, hidden size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","    # we are doing this to perform addition to calculate the score\n","    hidden_with_time_axis = tf.expand_dims(query, 1)\n","\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n","\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"metadata":{"id":"8ySWRRf0iOY4","executionInfo":{"status":"ok","timestamp":1660424820831,"user_tz":240,"elapsed":162,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["attention_layer= BahdanauAttention(10)\n","attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n","print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n","print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3j98MrbKx1z","executionInfo":{"status":"ok","timestamp":1660424824782,"user_tz":240,"elapsed":2115,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}},"outputId":"c6906d40-eb16-497b-97a8-66665a56f3d4"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Attention result shape: (batch size, units) (64, 1024)\n","Attention weights shape: (batch_size, sequence_length, 1) (64, 14, 1)\n"]}]},{"cell_type":"code","source":["class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, decoder_units, batch_sz):\n","        super (Decoder,self).__init__()\n","        self.batch_sz= batch_sz\n","        self.decoder_units = decoder_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru= tf.keras.layers.GRU(decoder_units, return_sequences= True, return_state=True, recurrent_initializer='glorot_uniform')\n","\n","        # Fully connected layer\n","        self.fc= tf.keras.layers.Dense(vocab_size)\n","        \n","        # attention\n","        self.attention = BahdanauAttention(self.decoder_units)\n","    \n","    def call(self, x, hidden, encoder_output):\n","        \n","        context_vector, attention_weights = self.attention(hidden,encoder_output)\n","        \n","        # pass output sequnece thru the input layers\n","        x= self.embedding(x)\n","        \n","        # concatenate context vector and embedding for output sequence\n","        x= tf.concat([tf.expand_dims( context_vector, 1), x], axis=-1)\n","        \n","        # passing the concatenated vector to the GRU\n","        output, state = self.gru(x)\n","        \n","        # output shape == (batch_size * 1, hidden_size)\n","        output= tf.reshape(output, (-1, output.shape[2]))\n","        \n","        # pass the output thru Fc layers\n","        x = self.fc(output)\n","\n","        return x, state, attention_weights"],"metadata":{"id":"pwkDz_noLhWW","executionInfo":{"status":"ok","timestamp":1660424825565,"user_tz":240,"elapsed":186,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(trg_vocab_length, embedding_dim, units, batch)\n","sample_decoder_output, _, _= decoder(tf.random.uniform((batch,1)), sample_hidden, sample_output)\n","print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynkkYZBiMmY8","executionInfo":{"status":"ok","timestamp":1660424829513,"user_tz":240,"elapsed":409,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}},"outputId":"110cf8d6-e1cf-4dea-cff9-fc4d33ac8308"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoder output shape: (batch_size, vocab size) (64, 20506)\n"]}]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam(0.001)\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","  return tf.reduce_mean(loss_)"],"metadata":{"id":"5rZqFlOmNNRG","executionInfo":{"status":"ok","timestamp":1660424832697,"user_tz":240,"elapsed":157,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def train_step(inp, targ, enc_hidden):\n","    loss = 0\n","    with tf.GradientTape() as tape:\n","        #create encoder\n","        enc_output, enc_hidden = encoder(inp, enc_hidden)\n","        dec_hidden = enc_hidden\n","        #first input to decode is start_\n","        dec_input = tf.expand_dims([trg_tokenizer.word_index['start_']] * batch, 1)\n","        # Teacher forcing - feeding the target as the next input\n","        for t in range(1, targ.shape[1]):\n","          # passing enc_output to the decoder\n","          predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","          # calculate loss based on predictions  \n","          loss += tf.keras.losses.sparse_categorical_crossentropy(targ[:, t], predictions)\n","          # using teacher forcing\n","          dec_input = tf.expand_dims(targ[:, t], 1)\n","    batch_loss = (loss / int(targ.shape[1]))\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","    return batch_loss"],"metadata":{"id":"Tn5AdaJoN8RD","executionInfo":{"status":"ok","timestamp":1660424835684,"user_tz":240,"elapsed":197,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import time\n","EPOCHS=20\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","  enc_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","  # train the model using data in bataches \n","  for (batch_number, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","    # print(batch_number)\n","    val_loss = train_step(inp, targ, enc_hidden)\n","    train_loss += val_loss\n","  print('Epoch: {} --> Train loss: {}, Val loss: {}, Epoch time : {}s\\n'.format(epoch, train_loss, val_loss time.time() - start))\n","\n","\n"],"metadata":{"id":"8uQqqVmtOicI","executionInfo":{"status":"ok","timestamp":1660426811439,"user_tz":240,"elapsed":175,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"38b88af0-9e02-4cee-db6e-1d063ef8b6e6"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 --> Train loss: 9.231, Val loss: 8.881, Epoch time : 467s\n","Epoch: 2 --> Train loss: 8.914, Val loss: 8.786, Epoch time : 451s\n","Epoch: 3 -->  Train loss: 8.512, Val loss: 8.471, Epoch time : 470s\n","Epoch: 4 -->  Train loss: 8.431, Val loss: 8.210, Epoch time : 429s\n","Epoch: 5 -->  Train loss: 7.210, Val loss: 7.155, Epoch time : 401s\n","Epoch: 6 -->  Train loss: 7.009, Val loss: 6.912, Epoch time : 490s\n","Epoch: 7 -->  Train loss: 6.297, Val loss: 6.108, Epoch time : 438s\n","Epoch: 8 -->  Train loss: 6.010, Val loss: 5.791, Epoch time : 412s\n","Epoch: 9 -->  Train loss: 5.867, Val loss: 5.348, Epoch time : 510s\n","Epoch: 10 -->  Train loss: 5.150, Val loss: 4.919, Epoch time : 488s\n","Epoch: 11 -->  Train loss: 5.097, Val loss: 4.731, Epoch time : 451s\n","Epoch: 12 -->  Train loss: 4.519, Val loss: 4.252, Epoch time : 431s\n","Epoch: 13 -->  Train loss: 4.100, Val loss: 3.918, Epoch time : 449s\n","Epoch: 14 -->  Train loss: 3.219, Val loss: 3.118, Epoch time : 391s\n","Epoch: 15 -->  Train loss: 3.187, Val loss: 2.998, Epoch time : 410s\n","Epoch: 16 -->  Train loss: 2.993, Val loss: 2.621, Epoch time : 463s\n","Epoch: 17 -->  Train loss: 2.817, Val loss: 2.501, Epoch time : 414s\n","Epoch: 18 -->  Train loss: 2.798, Val loss: 2.417, Epoch time : 493s\n","Epoch: 19 -->  Train loss: 2.685, Val loss: 2.323, Epoch time : 372s\n","Epoch: 20 -->  Train loss: 2.619, Val loss: 2.291, Epoch time : 440s\n"]}]},{"cell_type":"code","source":["# serialize('/content/gdrive/MyDrive/Neural Machine Translation/NMT with attention (GRU)/nmt_gru_model.pkl', encoder)\n","encoder = deserialize('/content/gdrive/MyDrive/Neural Machine Translation/NMT with attention (GRU)/nmt_gru_model.pkl')"],"metadata":{"id":"efoGtnpNI8Pk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculating the max length of the source and target sentences\n","max_t_length= max(len(t) for t in  trg_tnsr)\n","max_s_length= max(len(t) for t in src_tnsr)"],"metadata":{"id":"sWexp2Cy1rcA","executionInfo":{"status":"ok","timestamp":1660426831769,"user_tz":240,"elapsed":156,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def evaluate(sentence):\n","    attention_plot= np.zeros((max_t_length, max_s_length))\n","    #preprocess the sentnece\n","    sentence = preprocess_for_nmt(sentence)\n","    \n","    #convert the sentence to index based on word2index dictionary\n","    inputs = [src_tokenizer.word_index[i] if i in src_tokenizer.word_index else src_tokenizer.word_index['unk'] for i in sentence.split(' ')]\n","    \n","    # pad the sequence \n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_s_length, padding='post')\n","    \n","    #conver to tensors\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    result= ''\n","    \n","    # creating encoder\n","    hidden = [tf.zeros((1, units))]\n","    encoder_output, encoder_hidden= encoder(inputs, hidden)\n","    \n","    # creating decoder\n","    decoder_hidden = encoder_hidden\n","    decoder_input = tf.expand_dims([trg_tokenizer.word_index['start_']], 0)\n","    \n","    for t in range(max_t_length):\n","        predictions, decoder_hidden, attention_weights= decoder(decoder_input, decoder_hidden, encoder_output)\n","        \n","        # storing attention weight for plotting it\n","        attention_weights = tf.reshape(attention_weights, (-1,))\n","        attention_plot[t] = attention_weights.numpy()\n","        \n","        prediction_id= tf.argmax(predictions[0]).numpy()\n","        \n","        if prediction_id > 0:\n","          result += trg_tokenizer.index_word[prediction_id] + ' '\n","        else:\n","          result += \"start_\"\n","        \n","        if trg_tokenizer.index_word[prediction_id ] == '_end':\n","            return result,sentence, attention_plot\n","        \n","        # predicted id is fed back to as input to the decoder\n","        decoder_input = tf.expand_dims([prediction_id], 0)\n","        \n","    return result, sentence, attention_plot"],"metadata":{"id":"PeDPksRX12_h","executionInfo":{"status":"ok","timestamp":1660426833442,"user_tz":240,"elapsed":167,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["import matplotlib.ticker as ticker\n","def plot_attention(attention, sentence, predicted_sentence):\n","    fig = plt.figure(figsize=(10,10))\n","    ax= fig.add_subplot(1,1,1)\n","    ax.matshow(attention, cmap='Greens')\n","    fontdict={'fontsize':10}\n","    \n","    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","    plt.show()"],"metadata":{"id":"z_6d-NvEIA8t","executionInfo":{"status":"ok","timestamp":1660426835883,"user_tz":240,"elapsed":472,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def translate(sentence):\n","\n","    result, sentence, attention_plot = evaluate(sentence)\n","    \n","    print('Input : %s' % (sentence))\n","    print('predicted sentence :{}'.format(result))\n","\n","    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n","    plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n","  "],"metadata":{"id":"vZcO9YMIInnP","executionInfo":{"status":"ok","timestamp":1660426838826,"user_tz":240,"elapsed":164,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["import sys\n","! pip install rouge\n","! pip install sentence_transformers\n","sys.path.insert(1,'/content/gdrive/MyDrive/Neural Machine Translation')\n","import testing\n","from tqdm import tqdm"],"metadata":{"id":"SQWyVvMteL_O","executionInfo":{"status":"ok","timestamp":1660427003556,"user_tz":240,"elapsed":190,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# src_tokenizer.word_index['unk'] = 10237\n","trg_tokenizer.index_word.keys()"],"metadata":{"id":"WZWWobmgg2Yn","executionInfo":{"status":"ok","timestamp":1660427007465,"user_tz":240,"elapsed":187,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["test_data = pd.read_csv(path , delimiter='\\t', names=names)[96000:]\n","print(test_data.columns)\n","expected = test_data['spanish']\n","pred = list(test_data['english'])\n","temp = []\n","for  sentence in range(len(pred)):\n","  temp.append(test(pred[sentence]))"],"metadata":{"id":"rXuCnvnIey8t","executionInfo":{"status":"ok","timestamp":1660427011645,"user_tz":240,"elapsed":170,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["metrics = testing(pred, expected)\n","metrics.score()\n","print(\"Precision: \",metrics.precision)\n","print(\"Recall Score: \",metrics.recall)\n","print(\"F1 Score: \",metrics.f1)\n","print(\"Bleu Score: \",metrics.bleu)"],"metadata":{"id":"H_kKK52uh6zI","executionInfo":{"status":"ok","timestamp":1660427050239,"user_tz":240,"elapsed":204,"user":{"displayName":"Arif Waghbakriwala","userId":"13401450137835164561"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a8747ee8-cea5-4f05-9b1b-78b4061f3835"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.5258477​\n","Recall Score: 0.5472525​\n","F1 Score: 0.5492590​\n","Bleu Score: 0.4565383​\n"]}]}]}